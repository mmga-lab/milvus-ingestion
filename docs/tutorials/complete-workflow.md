# å®Œæ•´å·¥ä½œæµæ•™ç¨‹

ä»æ•°æ®ç”Ÿæˆåˆ° Milvus ç”Ÿäº§éƒ¨ç½²çš„ç«¯åˆ°ç«¯å®Œæ•´æµç¨‹ï¼Œæ¶µç›–å¼€å‘ã€æµ‹è¯•ã€é¢„å‘å¸ƒã€ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µã€‚

## ğŸ¯ å·¥ä½œæµæ¦‚è§ˆ

```mermaid
graph LR
    A[æ¨¡å¼è®¾è®¡] --> B[æ•°æ®ç”Ÿæˆ]
    B --> C[æœ¬åœ°éªŒè¯] 
    C --> D[ä¸Šä¼ å­˜å‚¨]
    D --> E[Milvuså¯¼å…¥]
    E --> F[æ€§èƒ½æµ‹è¯•]
    F --> G[ç”Ÿäº§éƒ¨ç½²]
    
    A1[å†…ç½®æ¨¡å¼] --> A
    A2[è‡ªå®šä¹‰æ¨¡å¼] --> A
    
    C --> C1[é¢„è§ˆæ£€æŸ¥]
    C --> C2[å°è§„æ¨¡æµ‹è¯•]
    
    D --> D1[S3/MinIO]
    
    E --> E1[ç›´æ¥æ’å…¥]
    E --> E2[æ‰¹é‡å¯¼å…¥]
    
    F --> F1[æœç´¢æ€§èƒ½]
    F --> F2[ç´¢å¼•æ•ˆæœ]
```

## ğŸ—ï¸ é˜¶æ®µä¸€ï¼šç¯å¢ƒå‡†å¤‡

### 1.1 å¼€å‘ç¯å¢ƒæ­å»º

```bash
# é¡¹ç›®å…‹éš†å’Œåˆå§‹åŒ–
git clone https://github.com/zilliz/milvus-fake-data.git
cd milvus-fake-data
pdm install

# å¯åŠ¨æœ¬åœ°æµ‹è¯•ç¯å¢ƒ
cd deploy/
docker-compose up -d

# éªŒè¯æœåŠ¡çŠ¶æ€
curl http://localhost:19530/healthz  # Milvus
curl http://localhost:9000/minio/health/live  # MinIO

# è®¾ç½®ç¯å¢ƒå˜é‡
export MILVUS_URI=http://127.0.0.1:19530
export MINIO_HOST=127.0.0.1
export MINIO_ACCESS_KEY=minioadmin
export MINIO_SECRET_KEY=minioadmin
export MINIO_BUCKET=milvus-data
```

### 1.2 é¡¹ç›®ç»“æ„è§„åˆ’

```bash
mkdir -p {schemas,data,scripts,configs}

# åˆ›å»ºå·¥ä½œç›®å½•ç»“æ„
project/
â”œâ”€â”€ schemas/          # è‡ªå®šä¹‰æ¨¡å¼æ–‡ä»¶
â”œâ”€â”€ data/            # ç”Ÿæˆçš„æ•°æ®é›†
â”œâ”€â”€ scripts/         # è‡ªåŠ¨åŒ–è„šæœ¬
â”œâ”€â”€ configs/         # ç¯å¢ƒé…ç½®
â””â”€â”€ docs/           # é¡¹ç›®æ–‡æ¡£
```

## ğŸ“‹ é˜¶æ®µäºŒï¼šæ¨¡å¼è®¾è®¡ä¸éªŒè¯

### 2.1 éœ€æ±‚åˆ†æ

å‡è®¾æˆ‘ä»¬è¦æ„å»ºä¸€ä¸ª**æ™ºèƒ½å®¢æœçŸ¥è¯†åº“**ç³»ç»Ÿï¼š

**ä¸šåŠ¡éœ€æ±‚:**
- å­˜å‚¨FAQæ–‡æ¡£å’Œç­”æ¡ˆ
- æ”¯æŒè¯­ä¹‰æœç´¢
- åŒ…å«åˆ†ç±»å’Œæ ‡ç­¾
- æ”¯æŒå¤šè¯­è¨€
- éœ€è¦ç”¨æˆ·åé¦ˆè¯„åˆ†

### 2.2 æ¨¡å¼è®¾è®¡

```bash
# åˆ›å»ºçŸ¥è¯†åº“æ¨¡å¼
cat > schemas/knowledge_base.json << 'EOF'
{
  "collection_name": "knowledge_base",
  "description": "æ™ºèƒ½å®¢æœçŸ¥è¯†åº“å‘é‡æœç´¢é›†åˆ",
  "fields": [
    {
      "name": "doc_id",
      "type": "Int64",
      "is_primary": true,
      "auto_id": true,
      "description": "æ–‡æ¡£å”¯ä¸€æ ‡è¯†"
    },
    {
      "name": "question",
      "type": "VarChar",
      "max_length": 500,
      "description": "ç”¨æˆ·é—®é¢˜"
    },
    {
      "name": "answer",
      "type": "VarChar", 
      "max_length": 2000,
      "description": "æ ‡å‡†ç­”æ¡ˆ"
    },
    {
      "name": "category",
      "type": "VarChar",
      "max_length": 100,
      "description": "é—®é¢˜åˆ†ç±»"
    },
    {
      "name": "tags",
      "type": "Array",
      "element_type": "VarChar",
      "max_capacity": 10,
      "max_length": 50,
      "description": "é—®é¢˜æ ‡ç­¾"
    },
    {
      "name": "language",
      "type": "VarChar",
      "max_length": 10,
      "description": "è¯­è¨€ä»£ç "
    },
    {
      "name": "confidence_score",
      "type": "Float",
      "min": 0.0,
      "max": 1.0,
      "description": "ç­”æ¡ˆç½®ä¿¡åº¦"
    },
    {
      "name": "last_updated",
      "type": "VarChar",
      "max_length": 20,
      "description": "æœ€åæ›´æ–°æ—¶é—´"
    },
    {
      "name": "is_active",
      "type": "Bool",
      "description": "æ˜¯å¦æ¿€æ´»"
    },
    {
      "name": "question_embedding",
      "type": "FloatVector",
      "dim": 768,
      "description": "é—®é¢˜è¯­ä¹‰å‘é‡"
    },
    {
      "name": "answer_embedding", 
      "type": "FloatVector",
      "dim": 768,
      "description": "ç­”æ¡ˆè¯­ä¹‰å‘é‡"
    },
    {
      "name": "metadata",
      "type": "JSON",
      "nullable": true,
      "description": "æ‰©å±•å…ƒæ•°æ®"
    }
  ]
}
EOF
```

### 2.3 æ¨¡å¼éªŒè¯

```bash
# éªŒè¯æ¨¡å¼æ ¼å¼
milvus-fake-data generate --schema schemas/knowledge_base.json --validate-only

# é¢„è§ˆæ•°æ®ç»“æ„
milvus-fake-data generate --schema schemas/knowledge_base.json --rows 10 --preview

# æŸ¥çœ‹è¯¦ç»†å­—æ®µä¿¡æ¯
milvus-fake-data schema help --field-type Array
milvus-fake-data schema help --field-type JSON
```

### 2.4 æ³¨å†Œæ¨¡å¼

```bash
# å°†æ¨¡å¼æ·»åŠ åˆ°ç®¡ç†åº“
milvus-fake-data schema add knowledge_base schemas/knowledge_base.json \
  --description "æ™ºèƒ½å®¢æœçŸ¥è¯†åº“æ¨¡å¼" \
  --tags "å®¢æœ,FAQ,è¯­ä¹‰æœç´¢,å¤šè¯­è¨€"

# éªŒè¯æ³¨å†ŒæˆåŠŸ
milvus-fake-data schema show knowledge_base
```

## ğŸ”¬ é˜¶æ®µä¸‰ï¼šæ•°æ®ç”Ÿæˆä¸éªŒè¯

### 3.1 å°è§„æ¨¡éªŒè¯æµ‹è¯•

```bash
# ç”Ÿæˆå°æ ·æœ¬æ•°æ®è¿›è¡ŒéªŒè¯
milvus-fake-data generate \
  --builtin knowledge_base \
  --rows 1000 \
  --out data/kb_sample \
  --seed 42

# æ£€æŸ¥ç”Ÿæˆç»“æœ
ls -la data/kb_sample/
head -n 5 data/kb_sample/meta.json
```

### 3.2 æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬

```bash
# åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬
cat > scripts/validate_data.py << 'EOF'
#!/usr/bin/env python3
"""æ•°æ®è´¨é‡éªŒè¯è„šæœ¬"""

import pandas as pd
import json
from pathlib import Path
import sys

def validate_knowledge_base_data(data_dir):
    """éªŒè¯çŸ¥è¯†åº“æ•°æ®è´¨é‡"""
    data_path = Path(data_dir)
    
    # è¯»å–æ•°æ®
    parquet_file = data_path / "data.parquet"
    meta_file = data_path / "meta.json"
    
    if not parquet_file.exists():
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return False
        
    df = pd.read_parquet(parquet_file)
    
    with open(meta_file) as f:
        meta = json.load(f)
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  - æ€»è¡Œæ•°: {len(df):,}")
    print(f"  - å­—æ®µæ•°: {len(df.columns)}")
    print(f"  - æ–‡ä»¶å¤§å°: {parquet_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    # åŸºç¡€éªŒè¯
    issues = []
    
    # æ£€æŸ¥ä¸»é”®å”¯ä¸€æ€§
    if df['doc_id'].duplicated().any():
        issues.append("âŒ ä¸»é”®å­˜åœ¨é‡å¤")
    else:
        print("âœ… ä¸»é”®å”¯ä¸€æ€§æ£€æŸ¥é€šè¿‡")
    
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    required_fields = ['question', 'answer', 'category']
    for field in required_fields:
        if df[field].isnull().any():
            issues.append(f"âŒ å­—æ®µ {field} å­˜åœ¨ç©ºå€¼")
        else:
            print(f"âœ… å­—æ®µ {field} å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
    
    # æ£€æŸ¥å‘é‡ç»´åº¦
    vector_fields = ['question_embedding', 'answer_embedding']
    for field in vector_fields:
        sample_vector = df[field].iloc[0]
        if len(sample_vector) != 768:
            issues.append(f"âŒ å‘é‡å­—æ®µ {field} ç»´åº¦é”™è¯¯: {len(sample_vector)}")
        else:
            print(f"âœ… å‘é‡å­—æ®µ {field} ç»´åº¦æ­£ç¡®: 768")
    
    # æ£€æŸ¥åˆ†ç±»åˆ†å¸ƒ
    category_dist = df['category'].value_counts()
    print(f"\nğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in category_dist.head(10).items():
        print(f"  - {category}: {count} ({count/len(df)*100:.1f}%)")
    
    # æ£€æŸ¥è¯­è¨€åˆ†å¸ƒ
    lang_dist = df['language'].value_counts()
    print(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in lang_dist.items():
        print(f"  - {lang}: {count} ({count/len(df)*100:.1f}%)")
    
    if issues:
        print(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
        return False
    else:
        print(f"\nâœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡!")
        return True

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python validate_data.py <data_directory>")
        sys.exit(1)
    
    success = validate_knowledge_base_data(sys.argv[1])
    sys.exit(0 if success else 1)
EOF

chmod +x scripts/validate_data.py
```

```bash
# è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
python scripts/validate_data.py data/kb_sample
```

### 3.3 ä¸­ç­‰è§„æ¨¡æµ‹è¯•

```bash
# ç”Ÿæˆä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œæ€§èƒ½æµ‹è¯•
milvus-fake-data generate \
  --builtin knowledge_base \
  --rows 50000 \
  --batch-size 10000 \
  --out data/kb_medium \
  --seed 42

# éªŒè¯æ•°æ®è´¨é‡
python scripts/validate_data.py data/kb_medium
```

## ğŸ“¤ é˜¶æ®µå››ï¼šå­˜å‚¨ä¸Šä¼ 

### 4.1 é…ç½®å­˜å‚¨ç¯å¢ƒ

```bash
# åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶
cat > configs/storage.env << 'EOF'
# MinIO/S3 é…ç½®
MINIO_ENDPOINT=http://localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=knowledge-base-data

# ç”Ÿäº§ç¯å¢ƒé…ç½® (æ³¨é‡Šæ‰ï¼Œç”Ÿäº§æ—¶å¯ç”¨)
# MINIO_ENDPOINT=https://s3.company.com
# MINIO_ACCESS_KEY=prod_access_key
# MINIO_SECRET_KEY=prod_secret_key
# MINIO_BUCKET=prod-milvus-data
EOF

# åŠ è½½é…ç½®
source configs/storage.env
```

### 4.2 ä¸Šä¼ æ•°æ®

```bash
# ä¸Šä¼ æµ‹è¯•æ•°æ®
milvus-fake-data upload data/kb_sample \
  s3://$MINIO_BUCKET/knowledge-base/v1.0/sample/ \
  --endpoint-url $MINIO_ENDPOINT \
  --access-key-id $MINIO_ACCESS_KEY \
  --secret-access-key $MINIO_SECRET_KEY

# ä¸Šä¼ ä¸­ç­‰è§„æ¨¡æ•°æ®
milvus-fake-data upload data/kb_medium \
  s3://$MINIO_BUCKET/knowledge-base/v1.0/medium/ \
  --endpoint-url $MINIO_ENDPOINT \
  --access-key-id $MINIO_ACCESS_KEY \
  --secret-access-key $MINIO_SECRET_KEY
```

### 4.3 éªŒè¯ä¸Šä¼ ç»“æœ

```bash
# éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸ
curl -X GET "$MINIO_ENDPOINT/$MINIO_BUCKET/knowledge-base/v1.0/sample/" \
  --user "$MINIO_ACCESS_KEY:$MINIO_SECRET_KEY"
```

## ğŸ—„ï¸ é˜¶æ®µäº”ï¼šMilvus å¯¼å…¥

### 5.1 å°è§„æ¨¡ç›´æ¥æ’å…¥æµ‹è¯•

```bash
# ç›´æ¥æ’å…¥å°è§„æ¨¡æ•°æ®è¿›è¡Œè¿æ¥æµ‹è¯•
milvus-fake-data to-milvus insert data/kb_sample \
  --uri $MILVUS_URI \
  --collection-name knowledge_base_test \
  --batch-size 1000

# éªŒè¯æ’å…¥ç»“æœ
curl -X POST "$MILVUS_URI/v1/vector/collections/knowledge_base_test/query" \
  -H "Content-Type: application/json" \
  -d '{
    "limit": 5,
    "output_fields": ["doc_id", "question", "category"]
  }'
```

### 5.2 ä¸­ç­‰è§„æ¨¡æ‰¹é‡å¯¼å…¥

```bash
# æ‰¹é‡å¯¼å…¥ä¸­ç­‰è§„æ¨¡æ•°æ®
milvus-fake-data to-milvus import \
  --local-path data/kb_medium \
  --s3-path knowledge-base/v1.0/medium/ \
  --bucket $MINIO_BUCKET \
  --endpoint-url $MINIO_ENDPOINT \
  --access-key-id $MINIO_ACCESS_KEY \
  --secret-access-key $MINIO_SECRET_KEY \
  --collection-name knowledge_base_staging \
  --wait \
  --timeout 600
```

### 5.3 å¯¼å…¥ç»“æœéªŒè¯

```bash
# åˆ›å»ºéªŒè¯è„šæœ¬
cat > scripts/verify_import.py << 'EOF'
#!/usr/bin/env python3
"""éªŒè¯ Milvus å¯¼å…¥ç»“æœ"""

from pymilvus import MilvusClient
import json
import sys

def verify_milvus_import(uri, collection_name, expected_count):
    """éªŒè¯ Milvus å¯¼å…¥ç»“æœ"""
    
    client = MilvusClient(uri=uri)
    
    try:
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collections = client.list_collections()
        if collection_name not in collections:
            print(f"âŒ é›†åˆ {collection_name} ä¸å­˜åœ¨")
            return False
        
        print(f"âœ… é›†åˆ {collection_name} å­˜åœ¨")
        
        # è·å–é›†åˆç»Ÿè®¡
        stats = client.get_collection_stats(collection_name)
        actual_count = stats["row_count"]
        
        print(f"ğŸ“Š é›†åˆç»Ÿè®¡:")
        print(f"  - é¢„æœŸè¡Œæ•°: {expected_count:,}")
        print(f"  - å®é™…è¡Œæ•°: {actual_count:,}")
        print(f"  - åŒ¹é…åº¦: {actual_count/expected_count*100:.1f}%")
        
        # æ£€æŸ¥è¡Œæ•°åŒ¹é…
        if actual_count != expected_count:
            print(f"âš ï¸  è¡Œæ•°ä¸åŒ¹é…")
            return False
        
        # æµ‹è¯•æŸ¥è¯¢
        results = client.query(
            collection_name=collection_name,
            limit=5,
            output_fields=["doc_id", "question", "category"]
        )
        
        print(f"\nğŸ” æŸ¥è¯¢æµ‹è¯• (å‰5æ¡):")
        for i, result in enumerate(results, 1):
            print(f"  {i}. ID:{result['doc_id']} | {result['category']} | {result['question'][:50]}...")
        
        # æµ‹è¯•å‘é‡æœç´¢
        search_results = client.search(
            collection_name=collection_name,
            data=[[0.1] * 768],  # éšæœºå‘é‡
            anns_field="question_embedding",
            limit=3,
            output_fields=["doc_id", "question"]
        )
        
        print(f"\nğŸ¯ å‘é‡æœç´¢æµ‹è¯•:")
        for i, result in enumerate(search_results[0], 1):
            print(f"  {i}. ç›¸ä¼¼åº¦:{result['distance']:.3f} | {result['entity']['question'][:50]}...")
        
        print(f"\nâœ… Milvus å¯¼å…¥éªŒè¯é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False
    finally:
        client.close()

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("ç”¨æ³•: python verify_import.py <milvus_uri> <collection_name> <expected_count>")
        sys.exit(1)
    
    uri = sys.argv[1]
    collection_name = sys.argv[2]
    expected_count = int(sys.argv[3])
    
    success = verify_milvus_import(uri, collection_name, expected_count)
    sys.exit(0 if success else 1)
EOF

chmod +x scripts/verify_import.py
```

```bash
# éªŒè¯å¯¼å…¥ç»“æœ
python scripts/verify_import.py $MILVUS_URI knowledge_base_staging 50000
```

## âš¡ é˜¶æ®µå…­ï¼šæ€§èƒ½ä¼˜åŒ–æµ‹è¯•

### 6.1 å¤§è§„æ¨¡æ•°æ®ç”Ÿæˆ

```bash
# ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†ç”¨äºæ€§èƒ½æµ‹è¯•
milvus-fake-data generate \
  --builtin knowledge_base \
  --rows 1000000 \
  --batch-size 50000 \
  --max-file-size 256 \
  --max-rows-per-file 500000 \
  --out data/kb_large \
  --seed 42

# éªŒè¯å¤§è§„æ¨¡æ•°æ®
python scripts/validate_data.py data/kb_large
```

### 6.2 é«˜æ€§èƒ½å¯¼å…¥

```bash
# ä¸Šä¼ å¤§è§„æ¨¡æ•°æ®
milvus-fake-data upload data/kb_large \
  s3://$MINIO_BUCKET/knowledge-base/v1.0/large/ \
  --endpoint-url $MINIO_ENDPOINT \
  --access-key-id $MINIO_ACCESS_KEY \
  --secret-access-key $MINIO_SECRET_KEY

# é«˜æ€§èƒ½æ‰¹é‡å¯¼å…¥
time milvus-fake-data to-milvus import \
  --local-path data/kb_large \
  --s3-path knowledge-base/v1.0/large/ \
  --bucket $MINIO_BUCKET \
  --endpoint-url $MINIO_ENDPOINT \
  --access-key-id $MINIO_ACCESS_KEY \
  --secret-access-key $MINIO_SECRET_KEY \
  --collection-name knowledge_base_performance \
  --wait \
  --timeout 1800
```

### 6.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
cat > scripts/benchmark.py << 'EOF'
#!/usr/bin/env python3
"""Milvus æ€§èƒ½åŸºå‡†æµ‹è¯•"""

from pymilvus import MilvusClient
import time
import random
import numpy as np
import statistics

def benchmark_search(uri, collection_name, num_queries=100):
    """æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    client = MilvusClient(uri=uri)
    
    print(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•: {collection_name}")
    print(f"  - æŸ¥è¯¢æ¬¡æ•°: {num_queries}")
    print(f"  - å‘é‡ç»´åº¦: 768")
    
    search_times = []
    
    for i in range(num_queries):
        # ç”ŸæˆéšæœºæŸ¥è¯¢å‘é‡
        query_vector = np.random.random(768).tolist()
        
        start_time = time.time()
        
        results = client.search(
            collection_name=collection_name,
            data=[query_vector],
            anns_field="question_embedding",
            limit=10,
            output_fields=["doc_id", "question", "category"]
        )
        
        search_time = time.time() - start_time
        search_times.append(search_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i+1}/{num_queries}")
    
    # ç»Ÿè®¡ç»“æœ
    avg_time = statistics.mean(search_times)
    median_time = statistics.median(search_times)
    min_time = min(search_times)
    max_time = max(search_times)
    p95_time = np.percentile(search_times, 95)
    p99_time = np.percentile(search_times, 99)
    
    print(f"\nğŸ“Š æœç´¢æ€§èƒ½ç»Ÿè®¡ (ms):")
    print(f"  - å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}")
    print(f"  - ä¸­ä½æ•°å“åº”æ—¶é—´: {median_time:.2f}")
    print(f"  - æœ€å°å“åº”æ—¶é—´: {min_time:.2f}")
    print(f"  - æœ€å¤§å“åº”æ—¶é—´: {max_time:.2f}")
    print(f"  - P95 å“åº”æ—¶é—´: {p95_time:.2f}")
    print(f"  - P99 å“åº”æ—¶é—´: {p99_time:.2f}")
    print(f"  - QPS (ä¼°ç®—): {1000/avg_time:.1f}")
    
    client.close()
    
    return {
        "avg_time_ms": avg_time,
        "qps": 1000/avg_time,
        "p95_time_ms": p95_time,
        "p99_time_ms": p99_time
    }

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python benchmark.py <milvus_uri> <collection_name>")
        sys.exit(1)
    
    uri = sys.argv[1]
    collection_name = sys.argv[2]
    
    benchmark_search(uri, collection_name)
EOF

chmod +x scripts/benchmark.py
```

```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
python scripts/benchmark.py $MILVUS_URI knowledge_base_performance
```

## ğŸš€ é˜¶æ®µä¸ƒï¼šç”Ÿäº§éƒ¨ç½²

### 7.1 ç”Ÿäº§é…ç½®ç®¡ç†

```bash
# åˆ›å»ºç”Ÿäº§é…ç½®
cat > configs/production.env << 'EOF'
# ç”Ÿäº§ç¯å¢ƒ Milvus é…ç½®
PROD_MILVUS_URI=https://milvus-cluster.company.com:19530
PROD_MILVUS_TOKEN=production_token_here
PROD_MILVUS_DB=knowledge_base

# ç”Ÿäº§ç¯å¢ƒå­˜å‚¨é…ç½®
PROD_S3_ENDPOINT=https://s3.company.com
PROD_S3_ACCESS_KEY=prod_access_key_here
PROD_S3_SECRET_KEY=prod_secret_key_here
PROD_S3_BUCKET=prod-knowledge-base
PROD_S3_REGION=us-west-2

# éƒ¨ç½²é…ç½®
COLLECTION_NAME=knowledge_base_prod
BACKUP_RETENTION_DAYS=30
MAX_CONCURRENT_IMPORTS=3
EOF
```

### 7.2 ç”Ÿäº§éƒ¨ç½²è„šæœ¬

```bash
# åˆ›å»ºç”Ÿäº§éƒ¨ç½²è„šæœ¬
cat > scripts/deploy_production.sh << 'EOF'
#!/bin/bash
"""ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬"""

set -euo pipefail

# åŠ è½½é…ç½®
source configs/production.env

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥å‰ç½®æ¡ä»¶
check_prerequisites() {
    log_info "æ£€æŸ¥å‰ç½®æ¡ä»¶..."
    
    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    required_vars=(
        "PROD_MILVUS_URI"
        "PROD_S3_ENDPOINT" 
        "PROD_S3_BUCKET"
        "COLLECTION_NAME"
    )
    
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var:-}" ]]; then
            log_error "ç¯å¢ƒå˜é‡ $var æœªè®¾ç½®"
            exit 1
        fi
    done
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if [[ ! -d "data/kb_large" ]]; then
        log_error "ç”Ÿäº§æ•°æ®ç›®å½•ä¸å­˜åœ¨: data/kb_large"
        exit 1
    fi
    
    log_info "å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡"
}

# å¤‡ä»½ç°æœ‰é›†åˆ
backup_existing_collection() {
    log_info "å¤‡ä»½ç°æœ‰é›†åˆ..."
    
    # è¿™é‡Œæ·»åŠ å¤‡ä»½é€»è¾‘
    # å®é™…å®ç°æ—¶å¯èƒ½éœ€è¦å¯¼å‡ºç°æœ‰æ•°æ®
    
    log_info "å¤‡ä»½å®Œæˆ"
}

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy_to_production() {
    log_info "å¼€å§‹ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²..."
    
    # ä¸Šä¼ æ•°æ®åˆ°ç”Ÿäº§å­˜å‚¨
    log_info "ä¸Šä¼ æ•°æ®åˆ°ç”Ÿäº§å­˜å‚¨..."
    milvus-fake-data upload data/kb_large \
        s3://$PROD_S3_BUCKET/knowledge-base/$(date +%Y%m%d_%H%M%S)/ \
        --endpoint-url $PROD_S3_ENDPOINT \
        --access-key-id $PROD_S3_ACCESS_KEY \
        --secret-access-key $PROD_S3_SECRET_KEY \
        --region $PROD_S3_REGION
    
    # å¯¼å…¥åˆ°ç”Ÿäº§ Milvus
    log_info "å¯¼å…¥æ•°æ®åˆ°ç”Ÿäº§ Milvus..."
    milvus-fake-data to-milvus import \
        --local-path data/kb_large \
        --s3-path knowledge-base/$(date +%Y%m%d_%H%M%S)/ \
        --bucket $PROD_S3_BUCKET \
        --endpoint-url $PROD_S3_ENDPOINT \
        --access-key-id $PROD_S3_ACCESS_KEY \
        --secret-access-key $PROD_S3_SECRET_KEY \
        --uri $PROD_MILVUS_URI \
        --token $PROD_MILVUS_TOKEN \
        --db-name $PROD_MILVUS_DB \
        --collection-name $COLLECTION_NAME \
        --drop-if-exists \
        --wait \
        --timeout 3600
    
    log_info "ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å®Œæˆ"
}

# éªŒè¯éƒ¨ç½²ç»“æœ
verify_deployment() {
    log_info "éªŒè¯ç”Ÿäº§éƒ¨ç½²..."
    
    # è¿è¡ŒéªŒè¯è„šæœ¬
    python scripts/verify_import.py \
        $PROD_MILVUS_URI \
        $COLLECTION_NAME \
        1000000
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    python scripts/benchmark.py \
        $PROD_MILVUS_URI \
        $COLLECTION_NAME
    
    log_info "éƒ¨ç½²éªŒè¯å®Œæˆ"
}

# ä¸»æµç¨‹
main() {
    log_info "å¼€å§‹ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æµç¨‹..."
    
    check_prerequisites
    backup_existing_collection
    deploy_to_production
    verify_deployment
    
    log_info "ğŸ‰ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æˆåŠŸå®Œæˆï¼"
}

# é”™è¯¯å¤„ç†
trap 'log_error "éƒ¨ç½²è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"; exit 1' ERR

# æ‰§è¡Œä¸»æµç¨‹
main "$@"
EOF

chmod +x scripts/deploy_production.sh
```

### 7.3 æ‰§è¡Œç”Ÿäº§éƒ¨ç½²

```bash
# æ³¨æ„ï¼šè¿™æ˜¯ç¤ºä¾‹ï¼Œå®é™…ç”Ÿäº§éƒ¨ç½²å‰è¯·ä»”ç»†æ£€æŸ¥æ‰€æœ‰é…ç½®
# scripts/deploy_production.sh
```

## ğŸ“Š é˜¶æ®µå…«ï¼šç›‘æ§ä¸ç»´æŠ¤

### 8.1 å¥åº·æ£€æŸ¥è„šæœ¬

```bash
# åˆ›å»ºå¥åº·æ£€æŸ¥è„šæœ¬
cat > scripts/health_check.py << 'EOF'
#!/usr/bin/env python3
"""ç”Ÿäº§ç¯å¢ƒå¥åº·æ£€æŸ¥"""

from pymilvus import MilvusClient
import time
import sys
import json
from datetime import datetime

def health_check(uri, token, collection_name):
    """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
    
    client = MilvusClient(uri=uri, token=token)
    
    health_status = {
        "timestamp": datetime.now().isoformat(),
        "collection": collection_name,
        "checks": {}
    }
    
    try:
        # 1. è¿æ¥æ£€æŸ¥
        start_time = time.time()
        collections = client.list_collections()
        connection_time = time.time() - start_time
        
        health_status["checks"]["connection"] = {
            "status": "OK",
            "response_time_ms": connection_time * 1000,
            "collections_count": len(collections)
        }
        
        # 2. é›†åˆå­˜åœ¨æ£€æŸ¥
        if collection_name in collections:
            health_status["checks"]["collection_exists"] = {"status": "OK"}
        else:
            health_status["checks"]["collection_exists"] = {"status": "FAIL", "error": "Collection not found"}
            return health_status
        
        # 3. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        stats = client.get_collection_stats(collection_name)
        row_count = stats["row_count"]
        
        health_status["checks"]["data_integrity"] = {
            "status": "OK",
            "row_count": row_count,
            "expected_min": 1000000  # é¢„æœŸæœ€å°è¡Œæ•°
        }
        
        if row_count < 1000000:
            health_status["checks"]["data_integrity"]["status"] = "WARN"
            health_status["checks"]["data_integrity"]["warning"] = "Row count below expected"
        
        # 4. æŸ¥è¯¢æ€§èƒ½æ£€æŸ¥
        start_time = time.time()
        results = client.query(
            collection_name=collection_name,
            limit=1,
            output_fields=["doc_id"]
        )
        query_time = time.time() - start_time
        
        health_status["checks"]["query_performance"] = {
            "status": "OK" if query_time < 0.1 else "WARN",
            "response_time_ms": query_time * 1000,
            "threshold_ms": 100
        }
        
        # 5. æœç´¢æ€§èƒ½æ£€æŸ¥
        start_time = time.time()
        search_results = client.search(
            collection_name=collection_name,
            data=[[0.1] * 768],
            anns_field="question_embedding",
            limit=5
        )
        search_time = time.time() - start_time
        
        health_status["checks"]["search_performance"] = {
            "status": "OK" if search_time < 0.05 else "WARN",
            "response_time_ms": search_time * 1000,
            "results_count": len(search_results[0]) if search_results else 0,
            "threshold_ms": 50
        }
        
        # æ€»ä½“çŠ¶æ€
        all_checks = [check["status"] for check in health_status["checks"].values()]
        if all(status == "OK" for status in all_checks):
            health_status["overall_status"] = "HEALTHY"
        elif any(status == "FAIL" for status in all_checks):
            health_status["overall_status"] = "UNHEALTHY"
        else:
            health_status["overall_status"] = "DEGRADED"
            
    except Exception as e:
        health_status["checks"]["connection"] = {
            "status": "FAIL",
            "error": str(e)
        }
        health_status["overall_status"] = "UNHEALTHY"
    
    finally:
        client.close()
    
    return health_status

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("ç”¨æ³•: python health_check.py <uri> <token> <collection_name>")
        sys.exit(1)
    
    uri = sys.argv[1]
    token = sys.argv[2]
    collection_name = sys.argv[3]
    
    status = health_check(uri, token, collection_name)
    
    # è¾“å‡º JSON æ ¼å¼ç»“æœ
    print(json.dumps(status, indent=2))
    
    # æ ¹æ®çŠ¶æ€è®¾ç½®é€€å‡ºç 
    if status["overall_status"] == "HEALTHY":
        sys.exit(0)
    elif status["overall_status"] == "DEGRADED":
        sys.exit(1)
    else:
        sys.exit(2)
EOF

chmod +x scripts/health_check.py
```

### 8.2 ç›‘æ§å’Œå‘Šè­¦

```bash
# åˆ›å»ºç›‘æ§è„šæœ¬
cat > scripts/monitor.sh << 'EOF'
#!/bin/bash
"""ç”Ÿäº§ç›‘æ§è„šæœ¬"""

source configs/production.env

# è¿è¡Œå¥åº·æ£€æŸ¥
python scripts/health_check.py \
    $PROD_MILVUS_URI \
    $PROD_MILVUS_TOKEN \
    $COLLECTION_NAME > health_status.json

# æ£€æŸ¥çŠ¶æ€å¹¶å‘é€å‘Šè­¦ï¼ˆå®é™…ç¯å¢ƒä¸­å¯ä»¥é›†æˆä¼ä¸šç›‘æ§ç³»ç»Ÿï¼‰
overall_status=$(cat health_status.json | jq -r '.overall_status')

case $overall_status in
    "HEALTHY")
        echo "âœ… ç³»ç»ŸçŠ¶æ€æ­£å¸¸"
        ;;
    "DEGRADED")
        echo "âš ï¸  ç³»ç»Ÿæ€§èƒ½é™çº§ï¼Œéœ€è¦å…³æ³¨"
        # è¿™é‡Œå¯ä»¥å‘é€å‘Šè­¦é€šçŸ¥
        ;;
    "UNHEALTHY")
        echo "âŒ ç³»ç»Ÿå¼‚å¸¸ï¼Œéœ€è¦ç«‹å³å¤„ç†"
        # è¿™é‡Œå¯ä»¥å‘é€ç´§æ€¥å‘Šè­¦
        ;;
esac

# ä¿å­˜å†å²è®°å½•
mkdir -p logs/
cp health_status.json logs/health_$(date +%Y%m%d_%H%M%S).json
EOF

chmod +x scripts/monitor.sh
```

## ğŸ”„ é˜¶æ®µä¹ï¼šæ•°æ®æ›´æ–°æµç¨‹

### 9.1 å¢é‡æ›´æ–°æµç¨‹

```bash
# åˆ›å»ºå¢é‡æ›´æ–°è„šæœ¬
cat > scripts/incremental_update.sh << 'EOF'
#!/bin/bash
"""å¢é‡æ•°æ®æ›´æ–°æµç¨‹"""

set -euo pipefail

source configs/production.env

# ç”Ÿæˆå¢é‡æ•°æ®
log_info() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFO: $1"
}

log_info "å¼€å§‹å¢é‡æ•°æ®æ›´æ–°..."

# ç”Ÿæˆæ–°çš„æ•°æ®ï¼ˆæ¨¡æ‹Ÿå¢é‡ï¼‰
milvus-fake-data generate \
    --builtin knowledge_base \
    --rows 100000 \
    --out data/kb_incremental_$(date +%Y%m%d) \
    --seed $(date +%s)

# ä¸Šä¼ å¢é‡æ•°æ®
log_info "ä¸Šä¼ å¢é‡æ•°æ®..."
milvus-fake-data upload data/kb_incremental_$(date +%Y%m%d) \
    s3://$PROD_S3_BUCKET/knowledge-base/incremental/$(date +%Y%m%d)/ \
    --endpoint-url $PROD_S3_ENDPOINT \
    --access-key-id $PROD_S3_ACCESS_KEY \
    --secret-access-key $PROD_S3_SECRET_KEY

# åˆ›å»ºæ–°çš„é›†åˆç‰ˆæœ¬
NEW_COLLECTION="${COLLECTION_NAME}_$(date +%Y%m%d)"

# å¯¼å…¥å¢é‡æ•°æ®åˆ°æ–°é›†åˆ
log_info "å¯¼å…¥åˆ°æ–°é›†åˆ: $NEW_COLLECTION"
milvus-fake-data to-milvus import \
    --local-path data/kb_incremental_$(date +%Y%m%d) \
    --s3-path knowledge-base/incremental/$(date +%Y%m%d)/ \
    --bucket $PROD_S3_BUCKET \
    --endpoint-url $PROD_S3_ENDPOINT \
    --access-key-id $PROD_S3_ACCESS_KEY \
    --secret-access-key $PROD_S3_SECRET_KEY \
    --uri $PROD_MILVUS_URI \
    --token $PROD_MILVUS_TOKEN \
    --collection-name $NEW_COLLECTION \
    --wait

log_info "å¢é‡æ›´æ–°å®Œæˆï¼Œæ–°é›†åˆ: $NEW_COLLECTION"
log_info "è¯·éªŒè¯æ–°é›†åˆåï¼Œæ‰‹åŠ¨åˆ‡æ¢ç”Ÿäº§æµé‡"
EOF

chmod +x scripts/incremental_update.sh
```

## ğŸ“ æ€»ç»“ä¸æœ€ä½³å®è·µ

### âœ… å®Œæˆçš„å·¥ä½œæµç¨‹

1. **ç¯å¢ƒå‡†å¤‡** - Docker ç¯å¢ƒã€é…ç½®ç®¡ç†
2. **æ¨¡å¼è®¾è®¡** - ä¸šåŠ¡éœ€æ±‚åˆ†æã€å­—æ®µè®¾è®¡ã€éªŒè¯
3. **æ•°æ®ç”Ÿæˆ** - å°/ä¸­/å¤§è§„æ¨¡æ•°æ®ç”Ÿæˆ
4. **è´¨é‡ä¿è¯** - æ•°æ®éªŒè¯ã€è´¨é‡æ£€æŸ¥è„šæœ¬
5. **å­˜å‚¨ç®¡ç†** - S3/MinIO ä¸Šä¼ ã€ç‰ˆæœ¬ç®¡ç†
6. **æ•°æ®å¯¼å…¥** - ç›´æ¥æ’å…¥ã€æ‰¹é‡å¯¼å…¥
7. **æ€§èƒ½æµ‹è¯•** - æœç´¢æ€§èƒ½ã€QPS åŸºå‡†æµ‹è¯•
8. **ç”Ÿäº§éƒ¨ç½²** - è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
9. **ç›‘æ§ç»´æŠ¤** - å¥åº·æ£€æŸ¥ã€å‘Šè­¦æœºåˆ¶
10. **å¢é‡æ›´æ–°** - æ•°æ®æ›´æ–°æµç¨‹

### ğŸ¯ å…³é”®æˆåŠŸå› ç´ 

1. **æ¸è¿›å¼éªŒè¯** - ä»å°è§„æ¨¡åˆ°å¤§è§„æ¨¡é€æ­¥éªŒè¯
2. **è‡ªåŠ¨åŒ–è„šæœ¬** - å‡å°‘äººå·¥é”™è¯¯ï¼Œæé«˜æ•ˆç‡
3. **è´¨é‡ä¿è¯** - æ¯ä¸ªé˜¶æ®µéƒ½æœ‰éªŒè¯æœºåˆ¶
4. **æ€§èƒ½ç›‘æ§** - å®æ—¶æŒæ¡ç³»ç»ŸçŠ¶æ€
5. **ç‰ˆæœ¬ç®¡ç†** - æ”¯æŒå›æ»šå’Œå¢é‡æ›´æ–°

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡å‚è€ƒ

| é˜¶æ®µ | æ•°æ®è§„æ¨¡ | é¢„æœŸæ—¶é—´ | å…³é”®æŒ‡æ ‡ |
|------|----------|----------|----------|
| ç”Ÿæˆ | 100ä¸‡è¡Œ | 5-10åˆ†é’Ÿ | >10K è¡Œ/ç§’ |
| ä¸Šä¼  | 500MB | 2-5åˆ†é’Ÿ | >2MB/ç§’ |
| å¯¼å…¥ | 100ä¸‡è¡Œ | 10-20åˆ†é’Ÿ | >1K è¡Œ/ç§’ |
| æœç´¢ | 768ç»´å‘é‡ | <50ms | >20 QPS |

### ğŸš€ åç»­æ‰©å±•

è¿™ä¸ªå®Œæ•´å·¥ä½œæµç¨‹å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ï¼š

- **CI/CD é›†æˆ** - ä¸ GitLab/GitHub Actions é›†æˆ
- **å¤šç¯å¢ƒç®¡ç†** - å¼€å‘/æµ‹è¯•/é¢„å‘å¸ƒ/ç”Ÿäº§ç¯å¢ƒ
- **ç›‘æ§é›†æˆ** - Prometheusã€Grafana ç›‘æ§
- **å¤‡ä»½æ¢å¤** - è‡ªåŠ¨å¤‡ä»½å’Œç¾éš¾æ¢å¤
- **A/B æµ‹è¯•** - æ”¯æŒå¤šç‰ˆæœ¬é›†åˆå¯¹æ¯”æµ‹è¯•

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„å·¥ä½œæµç¨‹ï¼Œä½ å¯ä»¥é«˜æ•ˆã€å¯é åœ°ç®¡ç†ä»æ•°æ®ç”Ÿæˆåˆ°ç”Ÿäº§éƒ¨ç½²çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸï¼